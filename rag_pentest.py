#!/usr/bin/env python3
"""
Module RAG Pentest pour Cyber-Agent
Base de connaissances offline : MITRE ATT&CK, OWASP, CVEs, Méthodologies
"""

import os
import json
from typing import List, Dict
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader
from langchain_core.documents import Document

class PentestRAG:
    def __init__(self, persist_dir: str = "~/cyber-agent/knowledge_db"):
        self.persist_dir = os.path.expanduser(persist_dir)
        self.embeddings = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",  # Léger, fonctionne offline après téléchargement
            cache_folder=os.path.expanduser("~/.cache/sentence_transformers")
        )
        
        self.db = None
        self._init_db()
    
    def _init_db(self):
        """Initialise ou charge la base vectorielle"""
        if os.path.exists(self.persist_dir) and os.listdir(self.persist_dir):
            print(f"[RAG] Chargement base existante: {self.persist_dir}")
            self.db = Chroma(
                persist_directory=self.persist_dir,
                embedding_function=self.embeddings
            )
        else:
            print(f"[RAG] Nouvelle base à: {self.persist_dir}")
            os.makedirs(self.persist_dir, exist_ok=True)
            self.db = None
    
    def ingest_documents(self, docs_path: str):
        """
        Ingestion de documents pentest
        Supporte: .md, .txt, .pdf
        """
        print(f"[RAG] Ingestion de: {docs_path}")
        
        docs_path = os.path.expanduser(docs_path)
        documents = []
        
        # Chargement selon type
        if os.path.isdir(docs_path):
            loader = DirectoryLoader(
                docs_path, 
                glob="**/*.md",
                loader_cls=TextLoader
            )
            documents.extend(loader.load())
            
            # PDFs
            pdf_loader = DirectoryLoader(
                docs_path,
                glob="**/*.pdf",
                loader_cls=PyPDFLoader
            )
            try:
                documents.extend(pdf_loader.load())
            except:
                pass  # PyPDF non installé ou pas de PDF
        
        elif docs_path.endswith('.pdf'):
            loader = PyPDFLoader(docs_path)
            documents = loader.load()
        else:
            loader = TextLoader(docs_path)
            documents = loader.load()
        
        print(f"[RAG] {len(documents)} documents chargés")
        
        # Split en chunks pour meilleure recherche
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n## ", "\n### ", "\n\n", "\n", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        print(f"[RAG] {len(chunks)} chunks créés")
        
        # Création/Update base
        if self.db is None:
            self.db = Chroma.from_documents(
                documents=chunks,
                embedding=self.embeddings,
                persist_directory=self.persist_dir
            )
        else:
            self.db.add_documents(chunks)
        
        self.db.persist()
        print(f"[RAG] Base sauvegardée - {len(chunks)} chunks indexés")
    
    def query(self, question: str, k: int = 4) -> Dict:
        """
        Recherche dans la base de connaissances
        Retourne les documents pertinents + contexte pour le LLM
        """
        if self.db is None:
            return {"error": "Base vide. Ingestez d'abord des documents."}
        
        print(f"[RAG] Recherche: {question[:50]}...")
        
        # Recherche sémantique
        docs = self.db.similarity_search_with_score(question, k=k)
        
        # Formatage résultats
        context = "\n\n".join([f"[Source {i+1}] {doc.page_content}" 
                              for i, (doc, score) in enumerate(docs)])
        
        sources = [{
            "content": doc.page_content[:200] + "...",
            "score": float(score),
            "source": doc.metadata.get('source', 'inconnu')
        } for doc, score in docs]
        
        return {
            "context": context,
            "sources": sources,
            "query": question
        }

    def list_sources(self):
        """Liste les sources dans la base"""
        if self.db is None:
            return []
        # Récupération métadonnées uniques
        data = self.db.get()
        sources = set()
        for meta in data.get('metadatas', []):
            if meta and 'source' in meta:
                sources.add(meta['source'])
        return list(sources)
